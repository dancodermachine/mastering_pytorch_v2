{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOjrsc30fRAwSsqocoNNCV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536mxh2GArXK",
        "outputId": "95e57b63-250e-436d-cb24-d72ad4b99a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2 in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2) (1.3.0)\n",
            "Requirement already satisfied: torchvision==0.17 in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision==0.17) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchvision==0.17) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->torchvision==0.17) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2\n",
        "!pip install torchvision==0.17"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import gc\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ezrp6UoTGeDz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro"
      ],
      "metadata": {
        "id": "W2HD1C4qZR-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])\n",
        "points"
      ],
      "metadata": {
        "id": "chFpnQYUHGXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fe22b9-7e5a-4d56-ca62-dfa550e969f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 4., 2., 1., 3., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Iw6GZsG4GP",
        "outputId": "a0167b60-5754-48e7-dd05-b55b7985753c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\n",
        "points.storage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW9-1TblG864",
        "outputId": "2a8ee42b-d354-4409-f793-f4d01b38f6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-aa051972579d>:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  points.storage()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1.0\n",
              " 4.0\n",
              " 2.0\n",
              " 1.0\n",
              " 3.0\n",
              " 5.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points.untyped_storage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U69C5iQkHOEP",
        "outputId": "365979ec-8019-4d44-e79f-9457e4a5c491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0\n",
              " 0\n",
              " 128\n",
              " 63\n",
              " 0\n",
              " 0\n",
              " 128\n",
              " 64\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 64\n",
              " 0\n",
              " 0\n",
              " 128\n",
              " 63\n",
              " 0\n",
              " 0\n",
              " 64\n",
              " 64\n",
              " 0\n",
              " 0\n",
              " 160\n",
              " 64\n",
              "[torch.storage.UntypedStorage(device=cpu) of size 24]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1gLlU6OHk-2",
        "outputId": "ab081fb3-f9a4-43a7-8f62-63ad58f1c954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points.storage_offset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjUNdWNAHry9",
        "outputId": "8bfaf3f6-d9d2-414f-c7b5-01b77a2757ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points[1].storage_offset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yJswW3AH2U6",
        "outputId": "fefd55d4-00a0-4348-e6f7-e42408c84aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points.stride()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-jzVrlcIHDt",
        "outputId": "0633a218-c2eb-485e-be2a-21f645e73fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data types\n",
        "PyTorch offers the following data types to be contained within tensors:\n",
        "* `torch.float32` or `torch.float` - 32-bit floating-point.\n",
        "* `torch.float64` or `torch.double` - 64-bit, double-precision floating-point.\n",
        "* `torch.float16` or `torch.half` - 16-bit, half-precision floating-point.\n",
        "* `torch.int8` - Signed 8-bit integers.\n",
        "* `torch.uint-8` - Unsigned 8-bit integers.\n",
        "* `torch.int16` or `torch.short` - Signed 16-bit integers.\n",
        "* `torch.int32` or `torch.int` - Signed 32-bit integers.\n",
        "* `torch.int32` or `torch.long` - Signed 64-bit integers."
      ],
      "metadata": {
        "id": "pWkJW3eSI3UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32, device='cpu')\n",
        "points"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kV39-5OIPD6",
        "outputId": "366d95b4-89fe-4bb6-a510-2ce1681f7fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points = points.to(device='cuda')"
      ],
      "metadata": {
        "id": "LiZseHdwKJ58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points.is_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C-YZkxlKNSB",
        "outputId": "f2addba4-45a6-44a8-dbe6-b56be9314ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3j_89MoM4T0",
        "outputId": "d0e01168-fef3-49e1-f19d-3d22597967c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "points = points.to(device='cpu')"
      ],
      "metadata": {
        "id": "B65j-y7INPEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points.is_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAx2NjQLNR_t",
        "outputId": "34a36307-e536-4bf7-e7a0-95923e2f5ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6scqlFASNH3q",
        "outputId": "a12cf22b-f643-425e-d4c1-1f578b91f306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 13 11:30:18 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0              27W /  70W |    121MiB / 15360MiB |      3%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Modules\n",
        "### a) `torch.nn`"
      ],
      "metadata": {
        "id": "6AaZGc-JM0IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.randn(256, 4) / math.sqrt(256)\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9sms9PYLXb8",
        "outputId": "c16c3272-1935-4410-fe86-16853c92de7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1141, -0.0786, -0.0948,  0.0550],\n",
              "        [-0.0444,  0.0109, -0.1042, -0.0126],\n",
              "        [-0.1262,  0.0910,  0.0135,  0.0386],\n",
              "        ...,\n",
              "        [ 0.0255, -0.1038,  0.0020, -0.0006],\n",
              "        [-0.0408, -0.0078, -0.0110,  0.0438],\n",
              "        [-0.0502,  0.2021,  0.0157, -0.0165]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We then ensure that the parameters of this neural network are trainable, that is, the numbers in the 256x4 matrix can be\n",
        "# tuned with the help of back propagation.\n",
        "weights.requires_grad_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXk4UlbhVNJD",
        "outputId": "fa57a00b-dc08-4cdb-9f52-395505713ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1141, -0.0786, -0.0948,  0.0550],\n",
              "        [-0.0444,  0.0109, -0.1042, -0.0126],\n",
              "        [-0.1262,  0.0910,  0.0135,  0.0386],\n",
              "        ...,\n",
              "        [ 0.0255, -0.1038,  0.0020, -0.0006],\n",
              "        [-0.0408, -0.0078, -0.0110,  0.0438],\n",
              "        [-0.0502,  0.2021,  0.0157, -0.0165]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we also add the bias weights for the 4-dimensiona output, and make these trainable too\n",
        "bias = torch.zeros(4, requires_grad=True)\n",
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLLTb78SVg3f",
        "outputId": "59fd3add-9b77-47ad-d51e-e226d5811a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can instead use nn.Linear(256, 4) to represent the same thing in PyTorch\n",
        "nn.Linear(256, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdq9JCg0V6zg",
        "outputId": "e72c140d-872a-4a6c-c795-9ce97be18b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=256, out_features=4, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss_funct = F.cross_entropy\n",
        "#loss = loss_funct(model(X), y)"
      ],
      "metadata": {
        "id": "zM2mQWCBXLLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) `torch.optim`"
      ],
      "metadata": {
        "id": "cGauD2A_bWwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#opt = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "AGCxBd3mazOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) `torch.utils.data`"
      ],
      "metadata": {
        "id": "Cv8bEVzncSb8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qqXB3ggbwDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Neural Network using PyTorch\n",
        "### a) Define Model Architecture"
      ],
      "metadata": {
        "id": "EVkbRfG8cpls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ConvNet](https://github.com/dancodermachine/mastering_pytorch_v2/blob/main/images/chap1_cnn.png?raw=True)"
      ],
      "metadata": {
        "id": "AbbSGHAz8hCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
        "    self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
        "    self.dp1 = nn.Dropout2d(0.10)\n",
        "    self.dp2 = nn.Dropout2d(0.25)\n",
        "    self.fc1 = nn.Linear(4608, 64)\n",
        "    self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cn1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.cn2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dp1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dp2(x)\n",
        "    x = self.fc2(x)\n",
        "    op = F.log_softmax(x, dim=1)\n",
        "    return op\n"
      ],
      "metadata": {
        "id": "5_IaDG1zctVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Define Training and Inference Routines"
      ],
      "metadata": {
        "id": "UCSWa5TgnErV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_dataloader, optim, epoch):\n",
        "  model.train()\n",
        "  for b_i, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    optim.zero_grad()\n",
        "    pred_prob = model(X)\n",
        "    loss = F.nll_loss(pred_prob, y)\n",
        "    loss.backward\n",
        "    optim.step()\n",
        "    if b_i % 10 == 0:\n",
        "      print(\"Epoch: {} [{}/{} ({:.0f}%)]\\t training loss {:.6f}\".format(epoch,\n",
        "                                                                        b_i * len(X), # Instances\n",
        "                                                                        len(train_dataloader.dataset),\n",
        "                                                                        100. * b_i / len(train_dataloader),\n",
        "                                                                        loss.item()))"
      ],
      "metadata": {
        "id": "Xttq6tMrm6fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_dataloader):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  success = 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "      X.to(device), y.to(device)\n",
        "      pred_prob = model(X)\n",
        "      loss += F.nll_loss(pred_prob, y, reduction='sum').item() # Loss summed across the batch\n",
        "      pred = pred_prob.argmax(dim=1, keepdim=True) # Use argmax to get the most likely prediction\n",
        "      success += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "  loss /= len(test_dataloader.dataset)\n",
        "\n",
        "  print(\"\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n\".format(loss,\n",
        "                                                                                           success,\n",
        "                                                                                           len(test_dataloader.dataset),\n",
        "                                                                                           100. * success / len(test_dataloader.dataset)))"
      ],
      "metadata": {
        "id": "gM3ELD2eobUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Create Data Loaders"
      ],
      "metadata": {
        "id": "gk6cfGAOqtRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean and std values are calculated as the mean of all pixel values of all images in the training dataset.\n",
        "train_dataloader = torch.utils.data.DataLoader(datasets.MNIST(\"../data\",\n",
        "                                                              train=True,\n",
        "                                                              download=True,\n",
        "                                                              transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                                                        transforms.Normalize((0.1302),\n",
        "                                                                                                             (0.3069))])),\n",
        "                                               batch_size=32,\n",
        "                                               shuffle=True)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(datasets.MNIST(\"../data\",\n",
        "                                                             train=False,\n",
        "                                                             download=True,\n",
        "                                                             transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                                                           transforms.Normalize((0.1302),\n",
        "                                                                                                                (0.3069))])),\n",
        "                                              batch_size=512,\n",
        "                                              shuffle=False)"
      ],
      "metadata": {
        "id": "2SrYsYYgpmoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Define Optimizer and Run Training Epochs"
      ],
      "metadata": {
        "id": "6O7shnljuGYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = ConvNet()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "CVBQ-4wJsNYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e) Model Training"
      ],
      "metadata": {
        "id": "OIl-DdIIutZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 3):\n",
        "  train(model, device, train_dataloader, optimizer, epoch)\n",
        "  test(model, device, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJTUqf4Hus5d",
        "outputId": "bdd58b35-cf83-4a2f-f5cc-344a75eccc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 [0/60000 (0%)]\t training loss 2.310609\n",
            "Epoch: 1 [320/60000 (1%)]\t training loss 2.301408\n",
            "Epoch: 1 [640/60000 (1%)]\t training loss 2.285816\n",
            "Epoch: 1 [960/60000 (2%)]\t training loss 2.315040\n",
            "Epoch: 1 [1280/60000 (2%)]\t training loss 2.338356\n",
            "Epoch: 1 [1600/60000 (3%)]\t training loss 2.304013\n",
            "Epoch: 1 [1920/60000 (3%)]\t training loss 2.290039\n",
            "Epoch: 1 [2240/60000 (4%)]\t training loss 2.289615\n",
            "Epoch: 1 [2560/60000 (4%)]\t training loss 2.277966\n",
            "Epoch: 1 [2880/60000 (5%)]\t training loss 2.299315\n",
            "Epoch: 1 [3200/60000 (5%)]\t training loss 2.310122\n",
            "Epoch: 1 [3520/60000 (6%)]\t training loss 2.275659\n",
            "Epoch: 1 [3840/60000 (6%)]\t training loss 2.318302\n",
            "Epoch: 1 [4160/60000 (7%)]\t training loss 2.310825\n",
            "Epoch: 1 [4480/60000 (7%)]\t training loss 2.302435\n",
            "Epoch: 1 [4800/60000 (8%)]\t training loss 2.310029\n",
            "Epoch: 1 [5120/60000 (9%)]\t training loss 2.282497\n",
            "Epoch: 1 [5440/60000 (9%)]\t training loss 2.320853\n",
            "Epoch: 1 [5760/60000 (10%)]\t training loss 2.330604\n",
            "Epoch: 1 [6080/60000 (10%)]\t training loss 2.322438\n",
            "Epoch: 1 [6400/60000 (11%)]\t training loss 2.307484\n",
            "Epoch: 1 [6720/60000 (11%)]\t training loss 2.291392\n",
            "Epoch: 1 [7040/60000 (12%)]\t training loss 2.285457\n",
            "Epoch: 1 [7360/60000 (12%)]\t training loss 2.305897\n",
            "Epoch: 1 [7680/60000 (13%)]\t training loss 2.314601\n",
            "Epoch: 1 [8000/60000 (13%)]\t training loss 2.328165\n",
            "Epoch: 1 [8320/60000 (14%)]\t training loss 2.305839\n",
            "Epoch: 1 [8640/60000 (14%)]\t training loss 2.317677\n",
            "Epoch: 1 [8960/60000 (15%)]\t training loss 2.301759\n",
            "Epoch: 1 [9280/60000 (15%)]\t training loss 2.312589\n",
            "Epoch: 1 [9600/60000 (16%)]\t training loss 2.304696\n",
            "Epoch: 1 [9920/60000 (17%)]\t training loss 2.327831\n",
            "Epoch: 1 [10240/60000 (17%)]\t training loss 2.324873\n",
            "Epoch: 1 [10560/60000 (18%)]\t training loss 2.300314\n",
            "Epoch: 1 [10880/60000 (18%)]\t training loss 2.322654\n",
            "Epoch: 1 [11200/60000 (19%)]\t training loss 2.321518\n",
            "Epoch: 1 [11520/60000 (19%)]\t training loss 2.316390\n",
            "Epoch: 1 [11840/60000 (20%)]\t training loss 2.304704\n",
            "Epoch: 1 [12160/60000 (20%)]\t training loss 2.309113\n",
            "Epoch: 1 [12480/60000 (21%)]\t training loss 2.315621\n",
            "Epoch: 1 [12800/60000 (21%)]\t training loss 2.287184\n",
            "Epoch: 1 [13120/60000 (22%)]\t training loss 2.306442\n",
            "Epoch: 1 [13440/60000 (22%)]\t training loss 2.290249\n",
            "Epoch: 1 [13760/60000 (23%)]\t training loss 2.307555\n",
            "Epoch: 1 [14080/60000 (23%)]\t training loss 2.294812\n",
            "Epoch: 1 [14400/60000 (24%)]\t training loss 2.321670\n",
            "Epoch: 1 [14720/60000 (25%)]\t training loss 2.307693\n",
            "Epoch: 1 [15040/60000 (25%)]\t training loss 2.298151\n",
            "Epoch: 1 [15360/60000 (26%)]\t training loss 2.290849\n",
            "Epoch: 1 [15680/60000 (26%)]\t training loss 2.330231\n",
            "Epoch: 1 [16000/60000 (27%)]\t training loss 2.313620\n",
            "Epoch: 1 [16320/60000 (27%)]\t training loss 2.319285\n",
            "Epoch: 1 [16640/60000 (28%)]\t training loss 2.315807\n",
            "Epoch: 1 [16960/60000 (28%)]\t training loss 2.292672\n",
            "Epoch: 1 [17280/60000 (29%)]\t training loss 2.314665\n",
            "Epoch: 1 [17600/60000 (29%)]\t training loss 2.308141\n",
            "Epoch: 1 [17920/60000 (30%)]\t training loss 2.309435\n",
            "Epoch: 1 [18240/60000 (30%)]\t training loss 2.301562\n",
            "Epoch: 1 [18560/60000 (31%)]\t training loss 2.296260\n",
            "Epoch: 1 [18880/60000 (31%)]\t training loss 2.334221\n",
            "Epoch: 1 [19200/60000 (32%)]\t training loss 2.321759\n",
            "Epoch: 1 [19520/60000 (33%)]\t training loss 2.272998\n",
            "Epoch: 1 [19840/60000 (33%)]\t training loss 2.320882\n",
            "Epoch: 1 [20160/60000 (34%)]\t training loss 2.290170\n",
            "Epoch: 1 [20480/60000 (34%)]\t training loss 2.291852\n",
            "Epoch: 1 [20800/60000 (35%)]\t training loss 2.305823\n",
            "Epoch: 1 [21120/60000 (35%)]\t training loss 2.305268\n",
            "Epoch: 1 [21440/60000 (36%)]\t training loss 2.311524\n",
            "Epoch: 1 [21760/60000 (36%)]\t training loss 2.307429\n",
            "Epoch: 1 [22080/60000 (37%)]\t training loss 2.303131\n",
            "Epoch: 1 [22400/60000 (37%)]\t training loss 2.307717\n",
            "Epoch: 1 [22720/60000 (38%)]\t training loss 2.293488\n",
            "Epoch: 1 [23040/60000 (38%)]\t training loss 2.339530\n",
            "Epoch: 1 [23360/60000 (39%)]\t training loss 2.292244\n",
            "Epoch: 1 [23680/60000 (39%)]\t training loss 2.321304\n",
            "Epoch: 1 [24000/60000 (40%)]\t training loss 2.305053\n",
            "Epoch: 1 [24320/60000 (41%)]\t training loss 2.308393\n",
            "Epoch: 1 [24640/60000 (41%)]\t training loss 2.307066\n",
            "Epoch: 1 [24960/60000 (42%)]\t training loss 2.296786\n",
            "Epoch: 1 [25280/60000 (42%)]\t training loss 2.302936\n",
            "Epoch: 1 [25600/60000 (43%)]\t training loss 2.288793\n",
            "Epoch: 1 [25920/60000 (43%)]\t training loss 2.319729\n",
            "Epoch: 1 [26240/60000 (44%)]\t training loss 2.283554\n",
            "Epoch: 1 [26560/60000 (44%)]\t training loss 2.306802\n",
            "Epoch: 1 [26880/60000 (45%)]\t training loss 2.311836\n",
            "Epoch: 1 [27200/60000 (45%)]\t training loss 2.291534\n",
            "Epoch: 1 [27520/60000 (46%)]\t training loss 2.314076\n",
            "Epoch: 1 [27840/60000 (46%)]\t training loss 2.335999\n",
            "Epoch: 1 [28160/60000 (47%)]\t training loss 2.326175\n",
            "Epoch: 1 [28480/60000 (47%)]\t training loss 2.306465\n",
            "Epoch: 1 [28800/60000 (48%)]\t training loss 2.323052\n",
            "Epoch: 1 [29120/60000 (49%)]\t training loss 2.318636\n",
            "Epoch: 1 [29440/60000 (49%)]\t training loss 2.322481\n",
            "Epoch: 1 [29760/60000 (50%)]\t training loss 2.299450\n",
            "Epoch: 1 [30080/60000 (50%)]\t training loss 2.317762\n",
            "Epoch: 1 [30400/60000 (51%)]\t training loss 2.284289\n",
            "Epoch: 1 [30720/60000 (51%)]\t training loss 2.298747\n",
            "Epoch: 1 [31040/60000 (52%)]\t training loss 2.316120\n",
            "Epoch: 1 [31360/60000 (52%)]\t training loss 2.296821\n",
            "Epoch: 1 [31680/60000 (53%)]\t training loss 2.335605\n",
            "Epoch: 1 [32000/60000 (53%)]\t training loss 2.314019\n",
            "Epoch: 1 [32320/60000 (54%)]\t training loss 2.319828\n",
            "Epoch: 1 [32640/60000 (54%)]\t training loss 2.310899\n",
            "Epoch: 1 [32960/60000 (55%)]\t training loss 2.282829\n",
            "Epoch: 1 [33280/60000 (55%)]\t training loss 2.319431\n",
            "Epoch: 1 [33600/60000 (56%)]\t training loss 2.315238\n",
            "Epoch: 1 [33920/60000 (57%)]\t training loss 2.315608\n",
            "Epoch: 1 [34240/60000 (57%)]\t training loss 2.310221\n",
            "Epoch: 1 [34560/60000 (58%)]\t training loss 2.310566\n",
            "Epoch: 1 [34880/60000 (58%)]\t training loss 2.309175\n",
            "Epoch: 1 [35200/60000 (59%)]\t training loss 2.319201\n",
            "Epoch: 1 [35520/60000 (59%)]\t training loss 2.317234\n",
            "Epoch: 1 [35840/60000 (60%)]\t training loss 2.260258\n",
            "Epoch: 1 [36160/60000 (60%)]\t training loss 2.283019\n",
            "Epoch: 1 [36480/60000 (61%)]\t training loss 2.338909\n",
            "Epoch: 1 [36800/60000 (61%)]\t training loss 2.312342\n",
            "Epoch: 1 [37120/60000 (62%)]\t training loss 2.276471\n",
            "Epoch: 1 [37440/60000 (62%)]\t training loss 2.301945\n",
            "Epoch: 1 [37760/60000 (63%)]\t training loss 2.317796\n",
            "Epoch: 1 [38080/60000 (63%)]\t training loss 2.299066\n",
            "Epoch: 1 [38400/60000 (64%)]\t training loss 2.319088\n",
            "Epoch: 1 [38720/60000 (65%)]\t training loss 2.303567\n",
            "Epoch: 1 [39040/60000 (65%)]\t training loss 2.313946\n",
            "Epoch: 1 [39360/60000 (66%)]\t training loss 2.299949\n",
            "Epoch: 1 [39680/60000 (66%)]\t training loss 2.305423\n",
            "Epoch: 1 [40000/60000 (67%)]\t training loss 2.319766\n",
            "Epoch: 1 [40320/60000 (67%)]\t training loss 2.312709\n",
            "Epoch: 1 [40640/60000 (68%)]\t training loss 2.266343\n",
            "Epoch: 1 [40960/60000 (68%)]\t training loss 2.329659\n",
            "Epoch: 1 [41280/60000 (69%)]\t training loss 2.313637\n",
            "Epoch: 1 [41600/60000 (69%)]\t training loss 2.309517\n",
            "Epoch: 1 [41920/60000 (70%)]\t training loss 2.318889\n",
            "Epoch: 1 [42240/60000 (70%)]\t training loss 2.315705\n",
            "Epoch: 1 [42560/60000 (71%)]\t training loss 2.329092\n",
            "Epoch: 1 [42880/60000 (71%)]\t training loss 2.281647\n",
            "Epoch: 1 [43200/60000 (72%)]\t training loss 2.316725\n",
            "Epoch: 1 [43520/60000 (73%)]\t training loss 2.295684\n",
            "Epoch: 1 [43840/60000 (73%)]\t training loss 2.296648\n",
            "Epoch: 1 [44160/60000 (74%)]\t training loss 2.288262\n",
            "Epoch: 1 [44480/60000 (74%)]\t training loss 2.290966\n",
            "Epoch: 1 [44800/60000 (75%)]\t training loss 2.315995\n",
            "Epoch: 1 [45120/60000 (75%)]\t training loss 2.308002\n",
            "Epoch: 1 [45440/60000 (76%)]\t training loss 2.301808\n",
            "Epoch: 1 [45760/60000 (76%)]\t training loss 2.287294\n",
            "Epoch: 1 [46080/60000 (77%)]\t training loss 2.318117\n",
            "Epoch: 1 [46400/60000 (77%)]\t training loss 2.337028\n",
            "Epoch: 1 [46720/60000 (78%)]\t training loss 2.302999\n",
            "Epoch: 1 [47040/60000 (78%)]\t training loss 2.326559\n",
            "Epoch: 1 [47360/60000 (79%)]\t training loss 2.332401\n",
            "Epoch: 1 [47680/60000 (79%)]\t training loss 2.289659\n",
            "Epoch: 1 [48000/60000 (80%)]\t training loss 2.332985\n",
            "Epoch: 1 [48320/60000 (81%)]\t training loss 2.328118\n",
            "Epoch: 1 [48640/60000 (81%)]\t training loss 2.306183\n",
            "Epoch: 1 [48960/60000 (82%)]\t training loss 2.318897\n",
            "Epoch: 1 [49280/60000 (82%)]\t training loss 2.303375\n",
            "Epoch: 1 [49600/60000 (83%)]\t training loss 2.286839\n",
            "Epoch: 1 [49920/60000 (83%)]\t training loss 2.289278\n",
            "Epoch: 1 [50240/60000 (84%)]\t training loss 2.335416\n",
            "Epoch: 1 [50560/60000 (84%)]\t training loss 2.330604\n",
            "Epoch: 1 [50880/60000 (85%)]\t training loss 2.304756\n",
            "Epoch: 1 [51200/60000 (85%)]\t training loss 2.306056\n",
            "Epoch: 1 [51520/60000 (86%)]\t training loss 2.310308\n",
            "Epoch: 1 [51840/60000 (86%)]\t training loss 2.317870\n",
            "Epoch: 1 [52160/60000 (87%)]\t training loss 2.274876\n",
            "Epoch: 1 [52480/60000 (87%)]\t training loss 2.303680\n",
            "Epoch: 1 [52800/60000 (88%)]\t training loss 2.302014\n",
            "Epoch: 1 [53120/60000 (89%)]\t training loss 2.310167\n",
            "Epoch: 1 [53440/60000 (89%)]\t training loss 2.328310\n",
            "Epoch: 1 [53760/60000 (90%)]\t training loss 2.311665\n",
            "Epoch: 1 [54080/60000 (90%)]\t training loss 2.319642\n",
            "Epoch: 1 [54400/60000 (91%)]\t training loss 2.331147\n",
            "Epoch: 1 [54720/60000 (91%)]\t training loss 2.319220\n",
            "Epoch: 1 [55040/60000 (92%)]\t training loss 2.306485\n",
            "Epoch: 1 [55360/60000 (92%)]\t training loss 2.320467\n",
            "Epoch: 1 [55680/60000 (93%)]\t training loss 2.332808\n",
            "Epoch: 1 [56000/60000 (93%)]\t training loss 2.272226\n",
            "Epoch: 1 [56320/60000 (94%)]\t training loss 2.307205\n",
            "Epoch: 1 [56640/60000 (94%)]\t training loss 2.281580\n",
            "Epoch: 1 [56960/60000 (95%)]\t training loss 2.321915\n",
            "Epoch: 1 [57280/60000 (95%)]\t training loss 2.313996\n",
            "Epoch: 1 [57600/60000 (96%)]\t training loss 2.284663\n",
            "Epoch: 1 [57920/60000 (97%)]\t training loss 2.324465\n",
            "Epoch: 1 [58240/60000 (97%)]\t training loss 2.288005\n",
            "Epoch: 1 [58560/60000 (98%)]\t training loss 2.312600\n",
            "Epoch: 1 [58880/60000 (98%)]\t training loss 2.293353\n",
            "Epoch: 1 [59200/60000 (99%)]\t training loss 2.297874\n",
            "Epoch: 1 [59520/60000 (99%)]\t training loss 2.279309\n",
            "Epoch: 1 [59840/60000 (100%)]\t training loss 2.294285\n",
            "\n",
            "Test dataset: Overall Loss: 2.3080, Overall Accuracy: 1220/10000 (12%)\n",
            "\n",
            "Epoch: 2 [0/60000 (0%)]\t training loss 2.300879\n",
            "Epoch: 2 [320/60000 (1%)]\t training loss 2.292662\n",
            "Epoch: 2 [640/60000 (1%)]\t training loss 2.300742\n",
            "Epoch: 2 [960/60000 (2%)]\t training loss 2.314463\n",
            "Epoch: 2 [1280/60000 (2%)]\t training loss 2.287591\n",
            "Epoch: 2 [1600/60000 (3%)]\t training loss 2.302261\n",
            "Epoch: 2 [1920/60000 (3%)]\t training loss 2.301155\n",
            "Epoch: 2 [2240/60000 (4%)]\t training loss 2.314180\n",
            "Epoch: 2 [2560/60000 (4%)]\t training loss 2.288537\n",
            "Epoch: 2 [2880/60000 (5%)]\t training loss 2.320419\n",
            "Epoch: 2 [3200/60000 (5%)]\t training loss 2.329358\n",
            "Epoch: 2 [3520/60000 (6%)]\t training loss 2.309355\n",
            "Epoch: 2 [3840/60000 (6%)]\t training loss 2.312144\n",
            "Epoch: 2 [4160/60000 (7%)]\t training loss 2.293213\n",
            "Epoch: 2 [4480/60000 (7%)]\t training loss 2.315165\n",
            "Epoch: 2 [4800/60000 (8%)]\t training loss 2.283556\n",
            "Epoch: 2 [5120/60000 (9%)]\t training loss 2.302961\n",
            "Epoch: 2 [5440/60000 (9%)]\t training loss 2.329310\n",
            "Epoch: 2 [5760/60000 (10%)]\t training loss 2.324637\n",
            "Epoch: 2 [6080/60000 (10%)]\t training loss 2.310535\n",
            "Epoch: 2 [6400/60000 (11%)]\t training loss 2.300139\n",
            "Epoch: 2 [6720/60000 (11%)]\t training loss 2.335547\n",
            "Epoch: 2 [7040/60000 (12%)]\t training loss 2.331524\n",
            "Epoch: 2 [7360/60000 (12%)]\t training loss 2.297028\n",
            "Epoch: 2 [7680/60000 (13%)]\t training loss 2.289274\n",
            "Epoch: 2 [8000/60000 (13%)]\t training loss 2.335281\n",
            "Epoch: 2 [8320/60000 (14%)]\t training loss 2.280089\n",
            "Epoch: 2 [8640/60000 (14%)]\t training loss 2.293197\n",
            "Epoch: 2 [8960/60000 (15%)]\t training loss 2.297344\n",
            "Epoch: 2 [9280/60000 (15%)]\t training loss 2.303759\n",
            "Epoch: 2 [9600/60000 (16%)]\t training loss 2.310404\n",
            "Epoch: 2 [9920/60000 (17%)]\t training loss 2.303180\n",
            "Epoch: 2 [10240/60000 (17%)]\t training loss 2.336905\n",
            "Epoch: 2 [10560/60000 (18%)]\t training loss 2.315129\n",
            "Epoch: 2 [10880/60000 (18%)]\t training loss 2.314399\n",
            "Epoch: 2 [11200/60000 (19%)]\t training loss 2.284028\n",
            "Epoch: 2 [11520/60000 (19%)]\t training loss 2.300186\n",
            "Epoch: 2 [11840/60000 (20%)]\t training loss 2.309943\n",
            "Epoch: 2 [12160/60000 (20%)]\t training loss 2.303854\n",
            "Epoch: 2 [12480/60000 (21%)]\t training loss 2.285769\n",
            "Epoch: 2 [12800/60000 (21%)]\t training loss 2.299303\n",
            "Epoch: 2 [13120/60000 (22%)]\t training loss 2.297451\n",
            "Epoch: 2 [13440/60000 (22%)]\t training loss 2.336725\n",
            "Epoch: 2 [13760/60000 (23%)]\t training loss 2.332167\n",
            "Epoch: 2 [14080/60000 (23%)]\t training loss 2.321960\n",
            "Epoch: 2 [14400/60000 (24%)]\t training loss 2.297644\n",
            "Epoch: 2 [14720/60000 (25%)]\t training loss 2.311233\n",
            "Epoch: 2 [15040/60000 (25%)]\t training loss 2.311626\n",
            "Epoch: 2 [15360/60000 (26%)]\t training loss 2.302105\n",
            "Epoch: 2 [15680/60000 (26%)]\t training loss 2.319636\n",
            "Epoch: 2 [16000/60000 (27%)]\t training loss 2.316347\n",
            "Epoch: 2 [16320/60000 (27%)]\t training loss 2.332353\n",
            "Epoch: 2 [16640/60000 (28%)]\t training loss 2.303345\n",
            "Epoch: 2 [16960/60000 (28%)]\t training loss 2.340861\n",
            "Epoch: 2 [17280/60000 (29%)]\t training loss 2.292469\n",
            "Epoch: 2 [17600/60000 (29%)]\t training loss 2.298625\n",
            "Epoch: 2 [17920/60000 (30%)]\t training loss 2.320397\n",
            "Epoch: 2 [18240/60000 (30%)]\t training loss 2.322631\n",
            "Epoch: 2 [18560/60000 (31%)]\t training loss 2.328022\n",
            "Epoch: 2 [18880/60000 (31%)]\t training loss 2.306417\n",
            "Epoch: 2 [19200/60000 (32%)]\t training loss 2.309781\n",
            "Epoch: 2 [19520/60000 (33%)]\t training loss 2.294111\n",
            "Epoch: 2 [19840/60000 (33%)]\t training loss 2.308667\n",
            "Epoch: 2 [20160/60000 (34%)]\t training loss 2.295998\n",
            "Epoch: 2 [20480/60000 (34%)]\t training loss 2.294997\n",
            "Epoch: 2 [20800/60000 (35%)]\t training loss 2.319951\n",
            "Epoch: 2 [21120/60000 (35%)]\t training loss 2.337056\n",
            "Epoch: 2 [21440/60000 (36%)]\t training loss 2.285900\n",
            "Epoch: 2 [21760/60000 (36%)]\t training loss 2.322243\n",
            "Epoch: 2 [22080/60000 (37%)]\t training loss 2.311956\n",
            "Epoch: 2 [22400/60000 (37%)]\t training loss 2.325286\n",
            "Epoch: 2 [22720/60000 (38%)]\t training loss 2.326449\n",
            "Epoch: 2 [23040/60000 (38%)]\t training loss 2.294221\n",
            "Epoch: 2 [23360/60000 (39%)]\t training loss 2.309603\n",
            "Epoch: 2 [23680/60000 (39%)]\t training loss 2.309740\n",
            "Epoch: 2 [24000/60000 (40%)]\t training loss 2.322407\n",
            "Epoch: 2 [24320/60000 (41%)]\t training loss 2.317698\n",
            "Epoch: 2 [24640/60000 (41%)]\t training loss 2.319794\n",
            "Epoch: 2 [24960/60000 (42%)]\t training loss 2.316947\n",
            "Epoch: 2 [25280/60000 (42%)]\t training loss 2.288260\n",
            "Epoch: 2 [25600/60000 (43%)]\t training loss 2.302719\n",
            "Epoch: 2 [25920/60000 (43%)]\t training loss 2.314324\n",
            "Epoch: 2 [26240/60000 (44%)]\t training loss 2.312106\n",
            "Epoch: 2 [26560/60000 (44%)]\t training loss 2.315359\n",
            "Epoch: 2 [26880/60000 (45%)]\t training loss 2.318200\n",
            "Epoch: 2 [27200/60000 (45%)]\t training loss 2.306534\n",
            "Epoch: 2 [27520/60000 (46%)]\t training loss 2.311537\n",
            "Epoch: 2 [27840/60000 (46%)]\t training loss 2.267325\n",
            "Epoch: 2 [28160/60000 (47%)]\t training loss 2.304800\n",
            "Epoch: 2 [28480/60000 (47%)]\t training loss 2.317432\n",
            "Epoch: 2 [28800/60000 (48%)]\t training loss 2.296668\n",
            "Epoch: 2 [29120/60000 (49%)]\t training loss 2.306057\n",
            "Epoch: 2 [29440/60000 (49%)]\t training loss 2.317161\n",
            "Epoch: 2 [29760/60000 (50%)]\t training loss 2.296082\n",
            "Epoch: 2 [30080/60000 (50%)]\t training loss 2.306490\n",
            "Epoch: 2 [30400/60000 (51%)]\t training loss 2.316367\n",
            "Epoch: 2 [30720/60000 (51%)]\t training loss 2.310428\n",
            "Epoch: 2 [31040/60000 (52%)]\t training loss 2.269546\n",
            "Epoch: 2 [31360/60000 (52%)]\t training loss 2.319855\n",
            "Epoch: 2 [31680/60000 (53%)]\t training loss 2.308158\n",
            "Epoch: 2 [32000/60000 (53%)]\t training loss 2.318133\n",
            "Epoch: 2 [32320/60000 (54%)]\t training loss 2.312509\n",
            "Epoch: 2 [32640/60000 (54%)]\t training loss 2.302514\n",
            "Epoch: 2 [32960/60000 (55%)]\t training loss 2.330226\n",
            "Epoch: 2 [33280/60000 (55%)]\t training loss 2.326313\n",
            "Epoch: 2 [33600/60000 (56%)]\t training loss 2.280102\n",
            "Epoch: 2 [33920/60000 (57%)]\t training loss 2.350295\n",
            "Epoch: 2 [34240/60000 (57%)]\t training loss 2.297204\n",
            "Epoch: 2 [34560/60000 (58%)]\t training loss 2.305827\n",
            "Epoch: 2 [34880/60000 (58%)]\t training loss 2.278421\n",
            "Epoch: 2 [35200/60000 (59%)]\t training loss 2.306065\n",
            "Epoch: 2 [35520/60000 (59%)]\t training loss 2.301864\n",
            "Epoch: 2 [35840/60000 (60%)]\t training loss 2.317249\n",
            "Epoch: 2 [36160/60000 (60%)]\t training loss 2.313298\n",
            "Epoch: 2 [36480/60000 (61%)]\t training loss 2.300573\n",
            "Epoch: 2 [36800/60000 (61%)]\t training loss 2.308623\n",
            "Epoch: 2 [37120/60000 (62%)]\t training loss 2.325179\n",
            "Epoch: 2 [37440/60000 (62%)]\t training loss 2.319200\n",
            "Epoch: 2 [37760/60000 (63%)]\t training loss 2.303799\n",
            "Epoch: 2 [38080/60000 (63%)]\t training loss 2.356268\n",
            "Epoch: 2 [38400/60000 (64%)]\t training loss 2.338183\n",
            "Epoch: 2 [38720/60000 (65%)]\t training loss 2.303095\n",
            "Epoch: 2 [39040/60000 (65%)]\t training loss 2.336040\n",
            "Epoch: 2 [39360/60000 (66%)]\t training loss 2.286918\n",
            "Epoch: 2 [39680/60000 (66%)]\t training loss 2.316652\n",
            "Epoch: 2 [40000/60000 (67%)]\t training loss 2.318944\n",
            "Epoch: 2 [40320/60000 (67%)]\t training loss 2.313464\n",
            "Epoch: 2 [40640/60000 (68%)]\t training loss 2.312254\n",
            "Epoch: 2 [40960/60000 (68%)]\t training loss 2.291927\n",
            "Epoch: 2 [41280/60000 (69%)]\t training loss 2.334305\n",
            "Epoch: 2 [41600/60000 (69%)]\t training loss 2.309068\n",
            "Epoch: 2 [41920/60000 (70%)]\t training loss 2.286201\n",
            "Epoch: 2 [42240/60000 (70%)]\t training loss 2.311981\n",
            "Epoch: 2 [42560/60000 (71%)]\t training loss 2.308045\n",
            "Epoch: 2 [42880/60000 (71%)]\t training loss 2.309348\n",
            "Epoch: 2 [43200/60000 (72%)]\t training loss 2.347471\n",
            "Epoch: 2 [43520/60000 (73%)]\t training loss 2.305739\n",
            "Epoch: 2 [43840/60000 (73%)]\t training loss 2.286795\n",
            "Epoch: 2 [44160/60000 (74%)]\t training loss 2.318761\n",
            "Epoch: 2 [44480/60000 (74%)]\t training loss 2.337776\n",
            "Epoch: 2 [44800/60000 (75%)]\t training loss 2.324618\n",
            "Epoch: 2 [45120/60000 (75%)]\t training loss 2.307114\n",
            "Epoch: 2 [45440/60000 (76%)]\t training loss 2.244349\n",
            "Epoch: 2 [45760/60000 (76%)]\t training loss 2.324213\n",
            "Epoch: 2 [46080/60000 (77%)]\t training loss 2.335416\n",
            "Epoch: 2 [46400/60000 (77%)]\t training loss 2.322874\n",
            "Epoch: 2 [46720/60000 (78%)]\t training loss 2.310053\n",
            "Epoch: 2 [47040/60000 (78%)]\t training loss 2.305072\n",
            "Epoch: 2 [47360/60000 (79%)]\t training loss 2.315471\n",
            "Epoch: 2 [47680/60000 (79%)]\t training loss 2.299383\n",
            "Epoch: 2 [48000/60000 (80%)]\t training loss 2.294934\n",
            "Epoch: 2 [48320/60000 (81%)]\t training loss 2.318449\n",
            "Epoch: 2 [48640/60000 (81%)]\t training loss 2.291049\n",
            "Epoch: 2 [48960/60000 (82%)]\t training loss 2.292250\n",
            "Epoch: 2 [49280/60000 (82%)]\t training loss 2.313900\n",
            "Epoch: 2 [49600/60000 (83%)]\t training loss 2.304680\n",
            "Epoch: 2 [49920/60000 (83%)]\t training loss 2.312485\n",
            "Epoch: 2 [50240/60000 (84%)]\t training loss 2.293040\n",
            "Epoch: 2 [50560/60000 (84%)]\t training loss 2.306670\n",
            "Epoch: 2 [50880/60000 (85%)]\t training loss 2.313397\n",
            "Epoch: 2 [51200/60000 (85%)]\t training loss 2.304628\n",
            "Epoch: 2 [51520/60000 (86%)]\t training loss 2.295475\n",
            "Epoch: 2 [51840/60000 (86%)]\t training loss 2.324650\n",
            "Epoch: 2 [52160/60000 (87%)]\t training loss 2.278857\n",
            "Epoch: 2 [52480/60000 (87%)]\t training loss 2.289470\n",
            "Epoch: 2 [52800/60000 (88%)]\t training loss 2.304799\n",
            "Epoch: 2 [53120/60000 (89%)]\t training loss 2.342125\n",
            "Epoch: 2 [53440/60000 (89%)]\t training loss 2.313575\n",
            "Epoch: 2 [53760/60000 (90%)]\t training loss 2.295444\n",
            "Epoch: 2 [54080/60000 (90%)]\t training loss 2.316424\n",
            "Epoch: 2 [54400/60000 (91%)]\t training loss 2.321840\n",
            "Epoch: 2 [54720/60000 (91%)]\t training loss 2.293767\n",
            "Epoch: 2 [55040/60000 (92%)]\t training loss 2.312201\n",
            "Epoch: 2 [55360/60000 (92%)]\t training loss 2.306316\n",
            "Epoch: 2 [55680/60000 (93%)]\t training loss 2.300537\n",
            "Epoch: 2 [56000/60000 (93%)]\t training loss 2.280564\n",
            "Epoch: 2 [56320/60000 (94%)]\t training loss 2.297978\n",
            "Epoch: 2 [56640/60000 (94%)]\t training loss 2.303644\n",
            "Epoch: 2 [56960/60000 (95%)]\t training loss 2.309690\n",
            "Epoch: 2 [57280/60000 (95%)]\t training loss 2.311634\n",
            "Epoch: 2 [57600/60000 (96%)]\t training loss 2.319925\n",
            "Epoch: 2 [57920/60000 (97%)]\t training loss 2.296930\n",
            "Epoch: 2 [58240/60000 (97%)]\t training loss 2.298630\n",
            "Epoch: 2 [58560/60000 (98%)]\t training loss 2.307407\n",
            "Epoch: 2 [58880/60000 (98%)]\t training loss 2.328983\n",
            "Epoch: 2 [59200/60000 (99%)]\t training loss 2.298842\n",
            "Epoch: 2 [59520/60000 (99%)]\t training loss 2.338927\n",
            "Epoch: 2 [59840/60000 (100%)]\t training loss 2.315560\n",
            "\n",
            "Test dataset: Overall Loss: 2.3080, Overall Accuracy: 1220/10000 (12%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVeXHZEKu3cb",
        "outputId": "d9d3d1a7-f9b1-4d4a-e9e3-582ffa682ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 13 11:31:35 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0              27W /  70W |    121MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### f) Run Inference on Trained Model"
      ],
      "metadata": {
        "id": "qiwXluhn16_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = enumerate(test_dataloader)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "\n",
        "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "5gCQedpSvLbF",
        "outputId": "2bb92971-6e85-4140-f36f-71f5db838e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
        "print(f\"Ground truth is : {sample_targets[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O13OGhZF2PYI",
        "outputId": "46940d46-2fca-4414-c705-2c4344c1b48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction is : 5\n",
            "Ground truth is : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtFyY-It2XsT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}